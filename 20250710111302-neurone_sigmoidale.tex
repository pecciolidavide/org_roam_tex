% Created 2026-02-07 Sat 19:32
% Intended LaTeX compiler: pdflatex
\documentclass[10pt]{article}
%% CREATO CON ORG - EMACS
\newcommand{\use}[2][]{\usepackage[#1]{#2}}
% PACCHETTI FONDAMENTLAI
\use[utf8]{inputenc}
\use[T1]{fontenc}
\use{graphicx}
\use{longtable}
\use{wrapfig}
\use{rotating}
\use[normalem]{ulem}
\use{amsmath}
\use{amsthm}
\use{amssymb}

\use{eucal} % Cambia mathcal{...}

\use{capt-of}
\use[italian]{babel}
\use[babel]{csquotes}
% bib la TEX lo carica in automatico org-cite
\use{microtype}
\use{lmodern}
\use{subfig} % sottofigure
\use{multicol} % due colonne
\use{lipsum} % lorem ipsum
\use{color} % colori in latex
\use{parskip} % rimuove l'indentazione dei nuovi paragrafi %% Add parbox=false to all new tcolorbox
\use{centernot}
\use[outline]{contour}\contourlength{3pt}
\use{fancyhdr}
\use{layout}
\use[most]{tcolorbox} % Riquadri colorati
\use{ifthen} % IFTHEN
\use{geometry}

% pacchetti matematica
\use{yhmath}
\use{dsfont}
\use{mathrsfs}
\use{cancel} % semplificare
\use{polynom} %divisione tra polinomi
\use{forest} % grafi ad albero
\use{booktabs} % tabelle
\use{commath} %simboli e differenziali
\use{bm} %bold
\use[fulladjust]{marginnote} %to use marginnote for date notes
\use{arrayjobx}%array
\use[intlimits]{empheq} % Riquadri colorati attorno alle equazioni
\use{mathtools}
\use{circuitikz} % Disegnare i circuiti
\use{mathtools}
\use{stmaryrd} % [[ \llbracket ]] \rrbracket
\use{bussproofs} % dimostrazioni

%%%%%%%%%%%%%


%%%% QUIVER
\newcommand{\duepunti}{\,\mathchar\numexpr"6000+`:\relax\,}
% A TikZ style for curved arrows of a fixed height, due to AndréC.
\tikzset{curve/.style={settings={#1},to path={(\tikztostart)
    .. controls ($(\tikztostart)!\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    and ($(\tikztostart)!1-\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    .. (\tikztotarget)\tikztonodes}},
    settings/.code={\tikzset{quiver/.cd,#1}
        \def\pv##1{\pgfkeysvalueof{/tikz/quiver/##1}}},
    quiver/.cd,pos/.initial=0.35,height/.initial=0}

% TikZ arrowhead/tail styles.
\tikzset{tail reversed/.code={\pgfsetarrowsstart{tikzcd to}}}
\tikzset{2tail/.code={\pgfsetarrowsstart{Implies[reversed]}}}
\tikzset{2tail reversed/.code={\pgfsetarrowsstart{Implies}}}
% TikZ arrow styles.
\tikzset{no body/.style={/tikz/dash pattern=on 0 off 1mm}}
%%%%%%%%%%


%% DEFINIZIONI COMANDI MATEMATICI
\let\sin\relax %TOGLIE LA DEFINIZIONE SU "\sin"

% cambia la definizione di empty set
% ---
\let\oldemptyset\emptyset
% ---
% \let\emptyset\varnothing
% ---
% \let\emptyset\relax
% \newcommand{\emptyset}{\text{\textnormal{\O}}}
% ---

\DeclareMathOperator{\bounded}{bd}
\DeclareMathOperator{\sin}{sen}
\DeclareMathOperator{\epi}{Epi}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\graph}{graph}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}
\DeclareMathOperator{\spettro}{Spettro}
\DeclareMathOperator{\nulls}{nullspace}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\ar}{ar}
\DeclareMathOperator{\const}{Const}
\DeclareMathOperator{\fun}{Fun}
\DeclareMathOperator{\rel}{Rel}
\DeclareMathOperator{\altezza}{ht}
\let\det\relax %TOGLIE LA DEFINIZIONE SU "\det"
\DeclareMathOperator{\det}{det}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\gl}{GL}
\def\Id{\mathrm{Id}}
\def\id{\mathrm{id}}
\DeclareMathOperator{\I}{\mathds{1}}
\DeclareMathOperator{\II}{II}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\tc}{t.c.}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\st}{st}
\DeclareMathOperator{\mon}{Mon}
\newcommand{\card}[1]{\left\vert #1 \right\vert}
\newcommand{\trasposta}[1]{\prescript{\text{T}}{}{#1}}
\newcommand{\1}{\mathds{1}}
\newcommand{\R}{\mathds{R}}
\newcommand{\diesis}{\#}
\newcommand{\bemolle}{\flat}
\newcommand{\nonstandard}[1]{\prescript{*}{}{#1}}
\newcommand{\starR}{\nonstandard{\R}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\lebesgue}[1]{\mathscr{L}\left(#1\right)}
\newcommand{\media}{\mathds{E}}
\newcommand{\K}{\mathds{K}}
\newcommand{\A}{\mathds{A}}
\newcommand{\Q}{\mathds{Q}}
\newcommand{\N}{\mathds{N}}
\newcommand{\C}{\mathds{C}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\qo}{\hspace{1em}\text{q.o.}\,}
\renewcommand{\tilde}[1]{\widetilde{#1}}
\renewcommand{\parallel}{\mathrel{/\mkern-5mu/}}
\newcommand{\parti}[2][]{\wp_{#1}(#2)}
\newcommand{\diff}[1]{\operatorname{d}_{#1}}
\let\oldvec\vec
\renewcommand{\vec}[1]{\overrightarrow{\vphantom{i}#1}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\dfreccia}[1]{\xrightarrow{\ #1 \ }}
\newcommand{\sfreccia}[1]{\xleftarrow{\ #1 \ }}
\newcommand{\formalsum}[2]{{\sum_{#1}^{#2}}{\vphantom{\sum}}'}
\newcommand{\minim}[2]{\mu_{#1}\, \left(#2\right)}
\newcommand{\concat}{\null^{\frown}} % concatenazione di stringe
\newcommand{\godelcode}[1]{\langle\!\langle #1 \rangle\!\rangle}
\newcommand{\godeldec}[1]{(\!(#1)\!)}
\newcommand{\termcode}[1]{\ulcorner #1\urcorner}
\newcommand{\partialto}{\dashrightarrow}
\newcommand{\restricted}{\upharpoonright}
\newcommand{\embeds}{\precsim}
\newcommand{\surjects}{\twoheadrightarrow}
\newcommand{\equipotenti}{\asymp}
%% \newcommand{\dotplus}{\mathbin{\dot{+}}} %% A quanto pare esiste già
\newcommand{\bigdot}{\mathbin{\boldsymbol{\cdot}}}
\newcommand{\dotexp}[1]{^{.#1}}
\newcommand{\conv}{\mathbin{*}}
\newcommand{\convolution}[2]{(#1\conv #2)}
\newcommand{\nil}{\mathfrak{N}}
\newcommand{\divisore}{\mathrel{|}}
\newcommand{\simplesso}[1]{\mathrm{e}_{#1}}

\renewcommand{\iff}{\mathrel{\longleftrightarrow}} %% Notazione Logica.
\newcommand{\oldiff}{\mathrel{\Longleftrightarrow}}
\renewcommand{\implies}{\mathrel{\rightarrow}} %% Notazione Logica
\newcommand{\oldimplies}{\mathrel{\Longrightarrow}}
\renewcommand{\impliedby}{\mathrel{\leftarrow}} %% Notazione Logica
\newcommand{\oldimpliedby}{\mathrel{\Longleftarrow}}

\newcommand{\IFF}{\quad\Longleftrightarrow\quad}
\newcommand{\IMPLICA}{\quad\Longrightarrow\quad}


\renewcommand{\descriptionlabel}[1]{\hspace{\labelsep}\normalfont #1} % remove bold from description


%% Definizione di Divergenza di K-L

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\kldiv}{D_{KL}\infdivx}

%% Definizione di \dotminus

\makeatletter
\newcommand{\dotminus}{\mathbin{\text{\@dotminus}}}

\newcommand{\@dotminus}{%
  \ooalign{\hidewidth\raise1ex\hbox{.}\hidewidth\cr$\m@th-$\cr}%
}
\makeatother

%tramite i prossimi due comandi posso decidere come scrivere i logaritmi naturali in tutti i documenti: ho infatti eliminato qualsiasi differenza tra "ln" e "log": se si vuole qualcosa di diverso bisogna inserire manualmente il tutto
\let\ln\relax
\DeclareMathOperator{\ln}{ln}
\let\log\relax
\DeclareMathOperator{\log}{log}
%%%%%%

%% NUOVI COMANDI
\newcommand{\straniero}[1]{\textit{#1}} %parole straniere
\newcommand{\titolo}[1]{\textsc{#1}} %titoli
\newcommand{\qedd}{\tag*{$\blacksquare$}} %qed per ambienti matemastici
\renewcommand{\qedsymbol}{$\blacksquare$} %modifica colore qed
\newcommand{\ooverline}[1]{\overline{\overline{#1}}}
\newcommand{\circoletto}[1]{\left(#1\right)^{\text{o}}}
%
\newcommand{\qmatrice}[1]{\begin{pmatrix}
#1_{11} & \cdots & #1_{1n}\\
\vdots & \ddots & \vdots \\
#1_{m1} & \cdots & #1_{mn}
\end{pmatrix}}
%
\newcommand{\parentesi}[2]{%
\underset{#1}{\underbrace{#2}}%
}
%
\newcommand{\norma}[1]{% Norma
\left\lVert#1\right\rVert%
}
\newcommand{\scalare}[2]{% Scalare
\left\langle #1, #2\right\rangle
}
%%%%%

%% RESTRIZIONI
\newcommand{\referenze}[2]{
        \phantomsection{}#2\textsuperscript{\textcolor{blue}{\textbf{#1}}}
}

\let\restriction\relax

\def\restriction#1#2{\mathchoice
              {\setbox1\hbox{${\displaystyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\textstyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptscriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}}
\def\restrictionaux#1#2{{#1\,\smash{\vrule height .8\ht1 depth .85\dp1}}_{\,#2}}
%%%%%%%%%%%

%%% FORMATTAZIONE FOOTNOTEMARK

\def\footnotemarkformatting#1{[#1]}
\renewcommand{\thefootnote}{\footnotemarkformatting{\arabic{footnote}}}

%% SEZIONE GRAFICA
\use{tikz}
\usetikzlibrary{matrix, patterns, calc, decorations.pathreplacing, hobby, decorations.markings, decorations.pathmorphing, babel}
\use{tikz-3dplot}
\use{mathrsfs} %per geogebra
\use{tikz-cd}
\tikzset
{
  %surface/.style={fill=black!10, shading=ball,fill opacity=0.4},
  plane/.style={black,pattern=north east lines},
  curve/.style={black,line width=0.5mm},
  dritto/.style={decoration={markings,mark=at position 0.5 with {\arrow{Stealth}}}, postaction=decorate},
  rovescio/.style={decoration={markings,mark=at position 0.5 with {\arrow{Stealth[reversed]}}}, postaction=decorate}
}
\use{pgfplots} % stampare le funzioni
        \pgfplotsset{/pgf/number format/use comma,compat=1.15}
        %\pgfplotsset{compat=1.15} %per geogebra
        \usepgfplotslibrary{fillbetween, polar}
%%%%%%

%% CITAZIONI
\use{lineno}

\newcommand{\citazione}[1]{%
  \begin{quotation}
  \begin{linenumbers}
  \modulolinenumbers[5]
  \begingroup
  \setlength{\parindent}{0cm}
  \noindent #1
  \endgroup
  \end{linenumbers}
  \end{quotation}\setcounter{linenumber}{1}
  }
%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% AMS THM

\theoremstyle{definition}% default
\newtheorem{thm}{Teorema}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposizione}
\newtheorem{cor}[thm]{Corollario}
\newtheorem{esempio}[thm]{Esempio}
\theoremstyle{plain}
\newtheorem{definizione}[thm]{Definizione}
\theoremstyle{remark}
\newtheorem*{oss}{Osservazione}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\use{hyperref}
\hypersetup{%
        pdfauthor={Davide Peccioli},
        pdfsubject={},
        allcolors=black,
        citecolor=black,
%	colorlinks=true,
        bookmarksopen=true}
\setcounter{secnumdepth}{0} % rimuove i numeri di sezione senza rimuovere le ref
\renewcommand{\href}[2]{\textcolor{blue}{#2}} % disabilita il comando href
\use{enotez} %
\setenotez{%
 mark-format = \footnotemarkformatting % Mette i numeri tra parentesi quadre%
}\let\footnote=\endnote % rende tutte le note a pié pagina come delle note a fine file 


\let\olddocument\document % modifico l'ambiende documenti per non dover stampare \printendnote
\let\oldenddocument\enddocument
\renewenvironment{document}%
{%
  \olddocument
}{%
  \printendnotes\oldenddocument
}
\renewcommand{\thethm}{\arabic{thm}}

\usepackage[hyperref]{biblatex}
\addbibresource{~/Documents/org/roam/bib/master.bib}
\author{Davide Peccioli}
\date{\today}
\title{}
\begin{document}

\section{Neurone Sigmoidale}
\label{sec:org147be77}
Il \href{20250624155858-neurone_artificiale.org}{neurone} sigmoidale è \(\langle\bm{x},\bm{y},\sigma,y\rangle\) dove
\begin{itemize}
\item \(\sigma\) è una \href{20250624155858-neurone_artificiale.org}{funzione di attivazione} \href{20250625110110-funzione_sigmoidale.org}{sigmoidale}, o direttamente la \href{20250624155858-neurone_artificiale.org}{sigmoide}
\begin{equation*}
  \sigma(t) = \frac{1}{1+e^{-t}}
\end{equation*}
\item \(y=\sigma(\bm{w}\cdot\bm{x}) = \sigma(\bm{w}\cdot\bm{x} - b)\).
\end{itemize}

Il vantaggio dei neuroni sigmoidali rispetto al percettrone è la continuità di \(\sigma\) (infatti la Heaviside non è continua): a piccole variazioni di input e pesi corrispondono piccole variazioni dell'output; nello specifico:
\begin{align*}
\dif y &= \sum_{i=1}^{n} \dpd{y}{w_{i}}\dif w_{i} + \dpd{y}{b}\dif b \\
&= \sum_{i=1}^{n} \sigma'(\bm{w}\cdot \bm{x}-b) x_{i}\dif w_{i} + \sigma'(\bm{w}\cdot \bm{x}-b) (-1)\dif b\\
&= \sigma'(\bm{w}\cdot \bm{x}-b) \cdot\left(
\sum_{i=1}^{n} x_{i}\dif w_{i} - \dif b
\right)
\end{align*}
Si noti inoltre che la derivata della sigmoide è maggiorata da \(1\).
\subsection{Regressione Logistica tramite neurone sigmoidale}
\label{sec:org76f266b}
Sia \(Z\) una variabile aleatoria, con immagine \(Z \in \set{-1,1}\), e sia \(X\) una variabile aleatoria discreta multidimensionale. Si vogliono approssimare le seguenti probabilità utilizzando un neurone sigmoidale:\footnote{Vedi ``\href{20250711174144-attesa_condizionata.org}{Probabilità condizionata}''}
\begin{equation*}
\mathds{P}(Z=1\mid X=\bm{x}),\qquad \mathds{P}(Z=-1\mid X=\bm{x}).
\end{equation*}

Si noti che
\begin{equation*}
\mathds{P}(Z=1\mid X=\bm{x})=1- \mathds{P}(Z=-1\mid X=\bm{x}).
\end{equation*}

Infatti, gli eventi \(Z=1\) e \(Z=-1\) ricoprono lo spazio di probabilità, pertanto
\begin{equation*}
\left(\set{Z=1}\cap\set{X=\bm{x}}\right)\cup \left(\set{Z=-1}\cap\set{X=\bm{x}}\right) = \set{X=x};
\end{equation*}
Esserendo \(Z=1\) e \(Z=-1\) due eventi disgunti, per la \(\sigma\)-additività:
\begin{align*}
\mathds{P}(Z=1 \land X=\bm{x})+\mathds{P}(Z=-1 \land X=\bm{x}) &= \mathds{P}(X=\bm{x})\\[.8em]
\frac{\mathds{P}(Z=1 \land X=\bm{x})+\mathds{P}(Z=-1 \land X=\bm{x})}{\mathds{P}(X=\bm{x})} &= 1\\[.6em]
\frac{\mathds{P}(Z=1 \land X=\bm{x})}{\mathds{P}(X=\bm{x})}+\frac{\mathds{P}(Z=-1 \land X=\bm{x})}{\mathds{P}(X=\bm{x})} &= 1
\end{align*}
Ricordando che l'attesa condizionata è, per \(\mathds{P}(B)\neq 0\), \(\mathds{P}(A\mid B) = \mathds{P}(A \land  B)/\mathds{P}(B)\), si ottiene
\begin{equation*}
\mathds{P}(Z=1\mid X=\bm{x})+ \mathds{P}(Z=-1\mid X=\bm{x})=1.
\end{equation*}

Si considera quindi \(\mathds{P}(Z=1\mid X=\bm{x})\) come una funzione di \(\bm{x}\), \(\mathds{P}(Z=1\mid X=\bm{x}) = f(\bm{x})\). È proprio questa funzione che si vuole approssimare con il neurone sigmoidale.

Dunque, se
\begin{equation*}
\sigma(t) = \frac{1}{1+e^{-t}},\qquad \sigma(t) = 1-\sigma(-t)
\end{equation*}
si cercano \(\bm{w}\) tali che \(\sigma(\bm{w}\cdot \bm{x}) \approx f(\bm{x})\). Infine, si noti che
\begin{align*}
\mathds{P}(Z=1\mid X=\bm{x}) &= \sigma(\bm{w}\cdot x)\\
\mathds{P}(Z=-1\mid X=\bm{x}) &= 1 - \mathds{P}(Z=1\mid X=\bm{x})\\ &= 1- \sigma(\bm{w}\cdot x) = \sigma(-\bm{w}\cdot\bm{x})
\end{align*}
e quindi si può scrivere \(\mathds{P}(Z=z\mid X=\bm{x})\approx\sigma(z\bm{w}\cdot\bm{x})\).

Data la coppia di v.a. \((X,Z)\) con legge \(p_{X,Z}\), si sono costruite due v.a. differenti:
\begin{enumerate}
\item \(\mathds{P}(Z\mid X)\), di legge \(p(\bm{x},z)\): \(p(X,Z)\);
\item la v.a. di legge \(\sigma(z\, \bm{w}\cdot \bm{x}) \eqqcolon p_{\bm{w}}(\bm{x},z)\): \(p_{\bm{w}}(X,Z)\)
\end{enumerate}
Chiaramente si vuole approssimare la prima utilizzando la seconda, minimizzando la funzione costo data dalla \href{20250711125821-teoria_dell_informazione_shannon.org}{divergenza di Kullback-Leibler}:
\begin{equation*}
C(\bm{w}) \coloneqq \kldiv{p(X,Z)}{p_{\bm{w}}(X,Z)} = S(p(X,Z), p_{\bm{w}}(X,Z)) - H(p(X,Z)).
\end{equation*}
Si è scelto \(\kldiv{p(X,Z)}{p_{\bm{w}}(X,Z)}\) invece di \(\kldiv{p_{\bm{w}}(X,Z)}{p(X,Z)}\) poiché \(H(p(X,Z))\) non dipende da \(\bm{w}\), e pertanto si può ignorare nel processo di \href{20250627153729-condizioni_necessarie_per_l_esistenza_di_un_minimo_di_una_funzione_reale.org}{minimizzazione} di \(C(\bm{w})\):
\begin{equation*}
C(\bm{w}) = S(p_{\bm{w}}(X,Z),p(X,Z)) = \media_{p(X,Z)}[-\ln p_{\bm{w}}(X,Z)] = \media_{p_{X,Z}}[-\ln p_{\bm{w}}(X,Z)].
\end{equation*}

Se quindi si conoscono \(N\) osservazioni di \((X,Z)\): \(\set{(\bm{x}_{i}, z_{i})\mid i=1,\dots,N}\), si può calcolare la versione empirica di \(C(\bm{w})\): \(\tilde{C}(\bm{w})\):
\begin{equation*}
\tilde{C}(\bm{w}) \coloneqq \cancel{\frac{1}{N}}\sum_{i=1}^{N} (-\ln p_{\bm{w}}(\bm{x}_{i},z_{i}))
\end{equation*}
dove \(1/N\) si è semplificato poiché valore fisso:
\begin{align*}
\tilde{C}(\bm{w}) &= - \sum_{i=1}^{N}\ln \sigma(z_{i}\, \bm{w}\cdot \bm{x}_{i})\\
&= \sum_{i=1}^{N} \ln(1+\exp(-z_{i}\, \bm{w}\cdot \bm{x}_{i})).
\end{align*}

È possibile quindi applicare l'algoritmo di \href{20250711122823-algoritmo_di_gradient_descent.org}{Gradient Descent} a \(\tilde{C}(\bm{w})\) per ottenere i valori ottimali di \(\bm{w}\).

Si noti inoltre che è consigliabile utilizzare questa funzione costo invece della solita \href{20250624155858-neurone_artificiale.org}{funzione costo} della \href{20250625123506-spazio_normato.org}{norma} \(L^{2}\), in quanto ha pendenza maggiore, e pertanto la convergenza del Gradient Descent è più veloce.

\uline{Nota}: questo metodo è equivalente alla massimizzazione della log-verosimiglianza, suppondendo che la v.a. \(\mathds{P}(Z\mid X)\) abbia distribuzione \(p_{\bm{w}}(\bm{x},z)\), di parametri \(\bm{w}\). Infatti, considerando delle osservazioni i.i.d. \((\bm{x}_{i},z_{i})\), la verosimiglianza è
\begin{equation*}
\mathcal{L}(\bm{w}) = \prod_{i=1}^{N} \mathds{P}(Z=z_{i}\mid X=x_{i}) = \prod_{i=1}^{N} \sigma(z_{i}\, \bm{w}\cdot \bm{x}_{i}).
\end{equation*}

Siccome massimizzare la verosimiglianza è equivalente a massimizzare la log-verosimiglianza, si massimizza
\begin{equation*}
\ell(\bm{w}) \coloneqq\ln\left(\prod_{i=1}^{N} \mathds{P}(Z=z_{i}\mid X=x_{i})\right) = \sum_{i=1}^{N}\ln\sigma(z_{i}\, \bm{w}\cdot\bm{x}_{i}) = -\tilde{C}(\bm{w}).
\end{equation*}
\end{document}
