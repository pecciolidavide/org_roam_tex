% Created 2026-02-07 Sat 19:30
% Intended LaTeX compiler: pdflatex
\documentclass[10pt]{article}
%% CREATO CON ORG - EMACS
\newcommand{\use}[2][]{\usepackage[#1]{#2}}
% PACCHETTI FONDAMENTLAI
\use[utf8]{inputenc}
\use[T1]{fontenc}
\use{graphicx}
\use{longtable}
\use{wrapfig}
\use{rotating}
\use[normalem]{ulem}
\use{amsmath}
\use{amsthm}
\use{amssymb}

\use{eucal} % Cambia mathcal{...}

\use{capt-of}
\use[italian]{babel}
\use[babel]{csquotes}
% bib la TEX lo carica in automatico org-cite
\use{microtype}
\use{lmodern}
\use{subfig} % sottofigure
\use{multicol} % due colonne
\use{lipsum} % lorem ipsum
\use{color} % colori in latex
\use{parskip} % rimuove l'indentazione dei nuovi paragrafi %% Add parbox=false to all new tcolorbox
\use{centernot}
\use[outline]{contour}\contourlength{3pt}
\use{fancyhdr}
\use{layout}
\use[most]{tcolorbox} % Riquadri colorati
\use{ifthen} % IFTHEN
\use{geometry}

% pacchetti matematica
\use{yhmath}
\use{dsfont}
\use{mathrsfs}
\use{cancel} % semplificare
\use{polynom} %divisione tra polinomi
\use{forest} % grafi ad albero
\use{booktabs} % tabelle
\use{commath} %simboli e differenziali
\use{bm} %bold
\use[fulladjust]{marginnote} %to use marginnote for date notes
\use{arrayjobx}%array
\use[intlimits]{empheq} % Riquadri colorati attorno alle equazioni
\use{mathtools}
\use{circuitikz} % Disegnare i circuiti
\use{mathtools}
\use{stmaryrd} % [[ \llbracket ]] \rrbracket
\use{bussproofs} % dimostrazioni

%%%%%%%%%%%%%


%%%% QUIVER
\newcommand{\duepunti}{\,\mathchar\numexpr"6000+`:\relax\,}
% A TikZ style for curved arrows of a fixed height, due to AndréC.
\tikzset{curve/.style={settings={#1},to path={(\tikztostart)
    .. controls ($(\tikztostart)!\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    and ($(\tikztostart)!1-\pv{pos}!(\tikztotarget)!\pv{height}!270:(\tikztotarget)$)
    .. (\tikztotarget)\tikztonodes}},
    settings/.code={\tikzset{quiver/.cd,#1}
        \def\pv##1{\pgfkeysvalueof{/tikz/quiver/##1}}},
    quiver/.cd,pos/.initial=0.35,height/.initial=0}

% TikZ arrowhead/tail styles.
\tikzset{tail reversed/.code={\pgfsetarrowsstart{tikzcd to}}}
\tikzset{2tail/.code={\pgfsetarrowsstart{Implies[reversed]}}}
\tikzset{2tail reversed/.code={\pgfsetarrowsstart{Implies}}}
% TikZ arrow styles.
\tikzset{no body/.style={/tikz/dash pattern=on 0 off 1mm}}
%%%%%%%%%%


%% DEFINIZIONI COMANDI MATEMATICI
\let\sin\relax %TOGLIE LA DEFINIZIONE SU "\sin"

% cambia la definizione di empty set
% ---
\let\oldemptyset\emptyset
% ---
% \let\emptyset\varnothing
% ---
% \let\emptyset\relax
% \newcommand{\emptyset}{\text{\textnormal{\O}}}
% ---

\DeclareMathOperator{\bounded}{bd}
\DeclareMathOperator{\sin}{sen}
\DeclareMathOperator{\epi}{Epi}
\DeclareMathOperator{\cl}{cl}
\DeclareMathOperator{\graph}{graph}
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}
\DeclareMathOperator{\spettro}{Spettro}
\DeclareMathOperator{\nulls}{nullspace}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\ar}{ar}
\DeclareMathOperator{\const}{Const}
\DeclareMathOperator{\fun}{Fun}
\DeclareMathOperator{\rel}{Rel}
\DeclareMathOperator{\altezza}{ht}
\let\det\relax %TOGLIE LA DEFINIZIONE SU "\det"
\DeclareMathOperator{\det}{det}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\gl}{GL}
\def\Id{\mathrm{Id}}
\def\id{\mathrm{id}}
\DeclareMathOperator{\I}{\mathds{1}}
\DeclareMathOperator{\II}{II}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\tc}{t.c.}
\DeclareMathOperator{\T}{T}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\st}{st}
\DeclareMathOperator{\mon}{Mon}
\newcommand{\card}[1]{\left\vert #1 \right\vert}
\newcommand{\trasposta}[1]{\prescript{\text{T}}{}{#1}}
\newcommand{\1}{\mathds{1}}
\newcommand{\R}{\mathds{R}}
\newcommand{\diesis}{\#}
\newcommand{\bemolle}{\flat}
\newcommand{\nonstandard}[1]{\prescript{*}{}{#1}}
\newcommand{\starR}{\nonstandard{\R}}
\newcommand{\borel}{\mathscr{B}}
\newcommand{\lebesgue}[1]{\mathscr{L}\left(#1\right)}
\newcommand{\media}{\mathds{E}}
\newcommand{\K}{\mathds{K}}
\newcommand{\A}{\mathds{A}}
\newcommand{\Q}{\mathds{Q}}
\newcommand{\N}{\mathds{N}}
\newcommand{\C}{\mathds{C}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\qo}{\hspace{1em}\text{q.o.}\,}
\renewcommand{\tilde}[1]{\widetilde{#1}}
\renewcommand{\parallel}{\mathrel{/\mkern-5mu/}}
\newcommand{\parti}[2][]{\wp_{#1}(#2)}
\newcommand{\diff}[1]{\operatorname{d}_{#1}}
\let\oldvec\vec
\renewcommand{\vec}[1]{\overrightarrow{\vphantom{i}#1}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\dfreccia}[1]{\xrightarrow{\ #1 \ }}
\newcommand{\sfreccia}[1]{\xleftarrow{\ #1 \ }}
\newcommand{\formalsum}[2]{{\sum_{#1}^{#2}}{\vphantom{\sum}}'}
\newcommand{\minim}[2]{\mu_{#1}\, \left(#2\right)}
\newcommand{\concat}{\null^{\frown}} % concatenazione di stringe
\newcommand{\godelcode}[1]{\langle\!\langle #1 \rangle\!\rangle}
\newcommand{\godeldec}[1]{(\!(#1)\!)}
\newcommand{\termcode}[1]{\ulcorner #1\urcorner}
\newcommand{\partialto}{\dashrightarrow}
\newcommand{\restricted}{\upharpoonright}
\newcommand{\embeds}{\precsim}
\newcommand{\surjects}{\twoheadrightarrow}
\newcommand{\equipotenti}{\asymp}
%% \newcommand{\dotplus}{\mathbin{\dot{+}}} %% A quanto pare esiste già
\newcommand{\bigdot}{\mathbin{\boldsymbol{\cdot}}}
\newcommand{\dotexp}[1]{^{.#1}}
\newcommand{\conv}{\mathbin{*}}
\newcommand{\convolution}[2]{(#1\conv #2)}
\newcommand{\nil}{\mathfrak{N}}
\newcommand{\divisore}{\mathrel{|}}
\newcommand{\simplesso}[1]{\mathrm{e}_{#1}}

\renewcommand{\iff}{\mathrel{\longleftrightarrow}} %% Notazione Logica.
\newcommand{\oldiff}{\mathrel{\Longleftrightarrow}}
\renewcommand{\implies}{\mathrel{\rightarrow}} %% Notazione Logica
\newcommand{\oldimplies}{\mathrel{\Longrightarrow}}
\renewcommand{\impliedby}{\mathrel{\leftarrow}} %% Notazione Logica
\newcommand{\oldimpliedby}{\mathrel{\Longleftarrow}}

\newcommand{\IFF}{\quad\Longleftrightarrow\quad}
\newcommand{\IMPLICA}{\quad\Longrightarrow\quad}


\renewcommand{\descriptionlabel}[1]{\hspace{\labelsep}\normalfont #1} % remove bold from description


%% Definizione di Divergenza di K-L

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\kldiv}{D_{KL}\infdivx}

%% Definizione di \dotminus

\makeatletter
\newcommand{\dotminus}{\mathbin{\text{\@dotminus}}}

\newcommand{\@dotminus}{%
  \ooalign{\hidewidth\raise1ex\hbox{.}\hidewidth\cr$\m@th-$\cr}%
}
\makeatother

%tramite i prossimi due comandi posso decidere come scrivere i logaritmi naturali in tutti i documenti: ho infatti eliminato qualsiasi differenza tra "ln" e "log": se si vuole qualcosa di diverso bisogna inserire manualmente il tutto
\let\ln\relax
\DeclareMathOperator{\ln}{ln}
\let\log\relax
\DeclareMathOperator{\log}{log}
%%%%%%

%% NUOVI COMANDI
\newcommand{\straniero}[1]{\textit{#1}} %parole straniere
\newcommand{\titolo}[1]{\textsc{#1}} %titoli
\newcommand{\qedd}{\tag*{$\blacksquare$}} %qed per ambienti matemastici
\renewcommand{\qedsymbol}{$\blacksquare$} %modifica colore qed
\newcommand{\ooverline}[1]{\overline{\overline{#1}}}
\newcommand{\circoletto}[1]{\left(#1\right)^{\text{o}}}
%
\newcommand{\qmatrice}[1]{\begin{pmatrix}
#1_{11} & \cdots & #1_{1n}\\
\vdots & \ddots & \vdots \\
#1_{m1} & \cdots & #1_{mn}
\end{pmatrix}}
%
\newcommand{\parentesi}[2]{%
\underset{#1}{\underbrace{#2}}%
}
%
\newcommand{\norma}[1]{% Norma
\left\lVert#1\right\rVert%
}
\newcommand{\scalare}[2]{% Scalare
\left\langle #1, #2\right\rangle
}
%%%%%

%% RESTRIZIONI
\newcommand{\referenze}[2]{
        \phantomsection{}#2\textsuperscript{\textcolor{blue}{\textbf{#1}}}
}

\let\restriction\relax

\def\restriction#1#2{\mathchoice
              {\setbox1\hbox{${\displaystyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\textstyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptscriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}}
\def\restrictionaux#1#2{{#1\,\smash{\vrule height .8\ht1 depth .85\dp1}}_{\,#2}}
%%%%%%%%%%%

%%% FORMATTAZIONE FOOTNOTEMARK

\def\footnotemarkformatting#1{[#1]}
\renewcommand{\thefootnote}{\footnotemarkformatting{\arabic{footnote}}}

%% SEZIONE GRAFICA
\use{tikz}
\usetikzlibrary{matrix, patterns, calc, decorations.pathreplacing, hobby, decorations.markings, decorations.pathmorphing, babel}
\use{tikz-3dplot}
\use{mathrsfs} %per geogebra
\use{tikz-cd}
\tikzset
{
  %surface/.style={fill=black!10, shading=ball,fill opacity=0.4},
  plane/.style={black,pattern=north east lines},
  curve/.style={black,line width=0.5mm},
  dritto/.style={decoration={markings,mark=at position 0.5 with {\arrow{Stealth}}}, postaction=decorate},
  rovescio/.style={decoration={markings,mark=at position 0.5 with {\arrow{Stealth[reversed]}}}, postaction=decorate}
}
\use{pgfplots} % stampare le funzioni
        \pgfplotsset{/pgf/number format/use comma,compat=1.15}
        %\pgfplotsset{compat=1.15} %per geogebra
        \usepgfplotslibrary{fillbetween, polar}
%%%%%%

%% CITAZIONI
\use{lineno}

\newcommand{\citazione}[1]{%
  \begin{quotation}
  \begin{linenumbers}
  \modulolinenumbers[5]
  \begingroup
  \setlength{\parindent}{0cm}
  \noindent #1
  \endgroup
  \end{linenumbers}
  \end{quotation}\setcounter{linenumber}{1}
  }
%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% AMS THM

\theoremstyle{definition}% default
\newtheorem{thm}{Teorema}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposizione}
\newtheorem{cor}[thm]{Corollario}
\newtheorem{esempio}[thm]{Esempio}
\theoremstyle{plain}
\newtheorem{definizione}[thm]{Definizione}
\theoremstyle{remark}
\newtheorem*{oss}{Osservazione}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\use{hyperref}
\hypersetup{%
        pdfauthor={Davide Peccioli},
        pdfsubject={},
        allcolors=black,
        citecolor=black,
%	colorlinks=true,
        bookmarksopen=true}
\setcounter{secnumdepth}{0} % rimuove i numeri di sezione senza rimuovere le ref
\renewcommand{\href}[2]{\textcolor{blue}{#2}} % disabilita il comando href
\use{enotez} %
\setenotez{%
 mark-format = \footnotemarkformatting % Mette i numeri tra parentesi quadre%
}\let\footnote=\endnote % rende tutte le note a pié pagina come delle note a fine file 


\let\olddocument\document % modifico l'ambiende documenti per non dover stampare \printendnote
\let\oldenddocument\enddocument
\renewenvironment{document}%
{%
  \olddocument
}{%
  \printendnotes\oldenddocument
}
\renewcommand{\thethm}{\arabic{thm}}

\usepackage[hyperref]{biblatex}
\addbibresource{~/Documents/org/roam/bib/master.bib}
\author{Davide Peccioli}
\date{\today}
\title{Metodi Matematici per il Machine Learning [CORSO]}
\begin{document}

EXPORT: \href{file:///Users/davidepeccioli/Documents/2. Areas/Matematica\_LM/Insegnamenti/Machine Learning/Note Personali/main.org}{main.org}
\section{Introduzione (De Rossi)}
\label{sec:orga93c914}

\begin{itemize}
\item \href{20250624155858-neurone_artificiale.org}{Neurone Artificiale}
\item \href{20250624155858-neurone_artificiale.org}{Funzioni di attivazione}
\item \href{20250624155858-neurone_artificiale.org}{Rete Neurale}
\item \href{20250624155858-neurone_artificiale.org}{Funzioni Costo (Machine Learning)}
\item \href{20250627110009-training_error_and_test_error.org}{Processo di apprendimento}
\item \href{20250714162717-approssimazione_per_regressione_lineare_machine_learning.org}{Approssimazione per Regressione Lineare (Machine Learning)}

\item \href{20250625104200-misura_di_baire.org}{Misura di Baire}
\item \href{20250625110110-funzione_sigmoidale.org}{Funzione sigmoidale}
\item \href{20250625105528-funzione_discriminatoria_per_una_misura_di_baire_sul_cubo_unitario.org}{Funzione discriminatoria per una misura di Baire sul cubo unitario}
\item \href{20250710102223-misura_di_baire_sul_cubo_nulla_se_nulla_su_tutti_i_semispazi.org}{Misura di Baire sul cubo nulla se nulla su tutti i semispazi}
\item \href{20250625110457-funzioni_sigmoidali_sono_discriminatorie_per_le_misure_di_baire_sul_cubo_unitario.org}{Funzioni sigmoidali sono discriminatorie per le misure di Baire sul cubo unitario}

\item \href{20250627153319-teorema_di_weierstrass.org}{Teorema di Weierstrass}
\item \href{20250627153729-condizioni_necessarie_per_l_esistenza_di_un_minimo_di_una_funzione_reale.org}{Minimizzazione di una funzione reale}

\item \href{20250627131207-curva_di_livello.org}{Curva di livello}
\item \href{20250627161722-hessiana_e_punti_stazionari_di_una_funzione_reale.org}{Hessiana e punti stazionari di una funzione reale}
\item \href{20250627130736-gradiente_e_perpendicolare_alle_curve_di_livello.org}{Gradiente è perpendicolare alle curve di livello}
\item \href{20250627130923-esistenza_di_una_curva_perpendicolare_a_tutte_le_curve_di_livello.org}{Esistenza di una curva perpendicolare a tutte le curve di livello}

\item \href{20250711122823-algoritmo_di_gradient_descent.org}{Metodo del Gradient Descent}
\item \href{20250716183256-metodo_dell_hessiana.org}{Metodo dell'Hessiana}
\item \href{20250716185823-metodo_di_newton.org}{Metodo di Newton}
\end{itemize}
\section{Analisi Matematica (Cordero)}
\label{sec:org91c0bc2}

\begin{itemize}
\item \href{20250629105513-teoremi_di_dini_per_la_convergenza_uniforme.org}{Teoremi di Dini per la convergenza uniforme}

\item \href{20250629110306-funzioni_uniformemente_limitate.org}{Funzioni uniformemente limitate}
\item \href{20250629113211-famiglia_di_funzioni_equicontinua.org}{Famiglia di funzioni equicontinua}
\item \href{20250629120441-teorema_di_ascoli_arzela.org}{Teorema di Ascoli-Arzelà}
\item \href{20250704170145-esistenza_di_una_funzione_continua_approssimabile_da_una_rete_neurale.org}{Rete Neurale può approssimare una funzione continua}

\item \href{20250629165520-algebra_di_funzioni_reali.org}{Algebra di funzioni reali}
\item \href{20250629151420-algebra_di_funzioni_separa_i_punti.org}{Algebra di funzioni separa i punti}
\item \href{20250629165421-teorema_di_stone_weierstrass.org}{Teorema di Stone-Weierstrass}
\item \href{20250629165421-teorema_di_stone_weierstrass.org}{Polinomi sono densi nelle funzioni continue}
\item \href{20250704170145-rete_neurale_che_approssima_funzioni_continue_periodiche.org}{Rete Neurale che approssima funzioni continue periodiche}

\item (\href{20250630103725-funzioni_approssimate_da_una_rete_neurale.org}{Funzioni approssimate da una rete neurale})

\item \href{20250630121612-wiener_s_tauberian_theorems.org}{Wiener's Tauberian Theorems}

\item \href{20250630155015-ogni_funzione_semplice_e_combinazione_lineare_di_heaviside.org}{Ogni funzione semplice è combinazione lineare di Heaviside}
\item \href{20250630155015-ogni_funzione_semplice_e_combinazione_lineare_di_heaviside.org}{Derivata distribuzionale di una funzione costante a tratti}
\item \href{20250630155139-misura_di_dirac_approssimata_da_misure_a_campana.org}{Approssimazione della misura di Dirac}
\item \href{20250630155208-funzione_reale_continua_su_un_compatto_e_uniformemente_continua.org}{Funzione reale continua su un compatto è uniformemente continua}

\item \href{20250630154418-one_hidden_layer_perceptron_network_impara_funzioni_continue.org}{One Hidden Layer Perceptron Network impara funzioni continue} (8.2.1)

\item \href{20250706121659-one_hidden_layer_sigmoidal_network.org}{One Hidden Layer Sigmoid Network impara funzioni continue} (8.3.1)

\item \href{20250706121659-funzioni_continue_approssimate_lineari_a_tratti.org}{Funzioni continue sono approssimate da funzioni lineari a tratti} (8.4.1)
\item \href{20250704170145-one_hidden_layer_relu_network_impara_funzioni_continue_unidimensionali.org}{One Hidden Layer ReLU Network impara funzioni continue} (8.4.3)

\item \href{20250704170145-one_hidden_layer_softplus_network_impara_funzioni_continue_unidimensionali.org}{Funzione softplus come convoluzione} (8.5.1)
\item \href{20250704170145-one_hidden_layer_softplus_network_impara_funzioni_continue_unidimensionali.org}{One Hidden Layer softplus Network impara funzioni continue} (8.5.7)

\item \href{20250630103725-funzioni_approssimate_da_una_rete_neurale.org}{Approssimazione Universale (Rete Neurale)} (9.2)
\item \href{20250704104537-spazio_delle_funzioni_misurabili_come_spazio_metrico.org}{Spazio delle funzioni misurabili come spazio metrico} (9.2.5)
\item \href{20250704152055-teorema_di_hahn_banach.org}{Teorema di Hahn-Banach} (Corollario 9.3.2)
\item \href{20250706121659-ohldn_f_continue.org}{One Hidden Layer Network impara funzioni continue sul cubo} (9.3.3, 9.3.5 \& 9.3.6)
\item \href{20250706121659-ohdspn_fun_cont.org}{One Hidden Layer SigmaPi-Network impara funzioni continue sui compatti} (9.3.8)
\item \href{20250625105528-funzione_discriminatoria_per_una_misura_di_baire_sul_cubo_unitario.org}{Funzione discriminatoria (Teoria della misura)} (9.3.10)
\item \href{20250706121659-ohldnl2.org}{One Hidden Layer Discriminatory Network impara funzioni L2 sul cubo} (9.3.11)
\item \href{20250704154416-funzione_l2_nulla_se_integrale_nullo_su_tutti_i_semispazi.org}{Funzione L2 nulla se integrale nullo su tutti i semispazi} (9.3.12)
\item \href{20250625105528-funzione_discriminatoria_per_una_misura_di_baire_sul_cubo_unitario.org}{Funzione discriminatoria (Teoria della misura)} (9.3.15 e 9.3.16)
\end{itemize}

Lezione 6:
\begin{itemize}
\item 9.3.17: \href{20250706121659-ohldnl1.org}{One Hidden Layer Discriminatory Network impara funzioni L1 sul cubo}
\item 9.3.20: \href{20250706105700-caratterizzazione_convergenza_in_misura.org}{Caratterizzazione convergenza in misura}
\item 9.3.21: \href{20250706110009-uniforme_convergenza_sui_compatti_implica_convergenza_in_misura.org}{Uniforme convergenza sui compatti implica convergenza in misura}
\item 9.3.22: \href{20250706121659-ohlsnfmc.org}{One Hidden Layer Sigmoidal Network impara funzioni misurabili sui compatti di Rn}
\item 9.4: \href{20250706110508-numero_di_neuroni_di_one_hidden_layer_network_e_accuratezza_dell_approssimazione.org}{Numero di neuroni di One Hidden Layer Network e accuratezza dell'approssimazione}
\item 9.5.1: \href{20250706110652-funzioni_continue_a_supporto_compatto_su_rn_sono_dense_in_lq.org}{Funzioni continue a supporto compatto su Rn sono dense in Lq}
\item 9.5.2: \href{20250706110741-funzioni_continue_e_lineari_a_tratti_sono_dense_nelle_funzioni_continue_a_supporto_compatto_su_rn.org}{Funzioni continue e lineari a tratti sono dense nelle funzioni continue a supporto compatto su Rn}
\item 9.5.3: \href{20250706121659-rnaqi.org}{Rete neurale che approssima funzioni Lq}
\end{itemize}

Lezione 7:
\href{20250708122736-exact_learning_machine_learning.org}{Exact Learning (Machine Learning)}
\begin{itemize}
\item 10.1.1: \href{20250708121822-one_hidden_layer_relu_network_impara_esattamente_funzioni_a_supporto_finito.org}{One Hidden Layer Perceptron Network impara esattamente funzioni a supporto finito}
\item 10.1.2: \href{20250708121822-one_hidden_layer_relu_network_impara_esattamente_funzioni_a_supporto_finito.org}{One Hidden Layer Perceptron Network impara esattamente funzioni a supporto finito}
\item 10.1.3: \href{20250708121822-one_hidden_layer_relu_network_impara_esattamente_funzioni_a_supporto_finito.org}{One Hidden Layer Perceptron Network impara esattamente funzioni a supporto finito}
\item 10.2.1: \href{20250624155858-neurone_artificiale.org}{Rete Neurale ReLU-feedforward}
\item 10.2.1 Representation of maxima: \href{20250708122516-rete_neurale_relu_feedforward_impara_esattamente_la_funzione_massimo.org}{Rete Neurale ReLU-feedforward impara esattamente la funzione massimo tra N input}
\item 10.2.2: \href{20250708122516-rete_neurale_relu_feedforward_impara_esattamente_la_funzione_massimo.org}{Rete Neurale ReLU-feedforward impara esattamente la funzione massimo tra N input}
\item 10.2.4: \href{20250708122516-rete_neurale_relu_feedforward_impara_esattamente_la_funzione_massimo.org}{Rete Neurale ReLU-feedforward impara esattamente la funzione massimo tra N input}
\end{itemize}

Lezione 8
\begin{itemize}
\item 10.2.5: \href{20250709134236-piu_profondita_in_favore_di_meno_larghezza_rete_neurale_relu_feedforward.org}{Più profondità in favore di meno larghezza - Rete Neurale ReLU-feedforward}
\item 10.2.6: \href{20250709134434-rete_neurale_relu_feedforward_ha_funzione_di_input_output_continua_e_lineare_a_tratti.org}{Rete Neurale ReLU-feedforward ha funzione di input-output continua e lineare a tratti}
\item 10.2.7 e 10.2.8: \href{20250709134700-rete_neurale_relu_feedforward_rappresenta_esattamente_funzioni_continue_lineari_a_tratti.org}{Rete Neurale ReLU-feedforward rappresenta esattamente funzioni continue lineari a tratti}
\item 10.3.1 e 10.3.2: \href{20250709135034-teorema_di_rappresentazione_di_kolmogorov_arnold.org}{Teorema di rappresentazione di Kolmogorov-Arnold}
\end{itemize}
\section{Cordero}
\label{sec:org96a822b}

\subsection{Lezione 1}
\label{sec:orgc682658}

\begin{itemize}
\item \href{20250624155858-neurone_artificiale.org}{Neurone Artificiale}
\item \href{20250710102223-percettrone.org}{Perceptron}
\end{itemize}
\subsection{Lezione 2}
\label{sec:org4caf02d}

\begin{itemize}
\item \href{20250710150636-bias_variance_tradeoff.org}{Bias-Variance Tradeoff}
\item \href{20250710111302-neurone_sigmoidale.org}{Neurone Sigmoidale}
\end{itemize}
\subsection{Lezione 3}
\label{sec:org2a25250}

\begin{itemize}
\item \href{20250710102223-percettrone.org}{Perceptron Learning Algorithm}
\end{itemize}
\subsection{Lezione 4}
\label{sec:orgb19faab}

\begin{itemize}
\item \href{20250711122025-subderivata.org}{Subderivata}
\item \href{20250711122025-subderivata.org}{Subdifferenziale}
\item \href{20250711122025-subderivata.org}{Subgradiente}
\item \href{20250711122823-metodo_del_subgradiente.org}{Metodo del subgradiente}
\item \href{20250711122823-algoritmo_di_gradient_descent.org}{Algoritmo di Stochastic Gradient Descent}
\item \href{20250711174144-attesa_condizionata.org}{Attesa condizionata}
\end{itemize}
\subsection{Lezione 5}
\label{sec:org7415837}

\begin{itemize}
\item \href{20250710111302-neurone_sigmoidale.org}{Neurone Sigmoidale}
\item \href{20250711125821-teoria_dell_informazione_shannon.org}{Teoria dell'informazione (Shannon)}
\item \href{20250710111302-neurone_sigmoidale.org}{Neurone Sigmoidale}
\end{itemize}
\subsection{Lezione 6}
\label{sec:org1ca5db2}

\begin{itemize}
\item \href{20250713214832-neurone_ad_input_continuo.org}{Neurone ad input continuo}
\item \href{20250624155858-neurone_artificiale.org}{Rete Neurale Feedforward}
\item \href{20250714105222-rete_neurale_che_implementa_lo_xor.org}{Rete Neurale che implementa lo XOR}
\item \href{20250713215224-backpropagation_per_una_rete_neurale.org}{Backpropagation per una rete neurale}
\end{itemize}
\subsection{Lezione 7}
\label{sec:orgebe3187}

\begin{itemize}
\item \href{20250713215224-backpropagation_per_una_rete_neurale.org}{Backpropagation per una rete neurale}
\end{itemize}
\subsection{Lezione 8}
\label{sec:org5c5c192}

\begin{itemize}
\item \href{20250713221021-pooling.org}{Pooling}
\item \href{20250714162717-sigma_algebra_generata_dal_massimo_di_variabili_aleatorie.org}{Sigma-algebra generata dal massimo di variabili aleatorie}
\item \href{20250713221345-rete_di_convoluzione.org}{Rete di Convoluzione}
\end{itemize}
\end{document}
